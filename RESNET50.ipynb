{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12658ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:10.994073Z",
     "iopub.status.busy": "2025-05-16T17:21:10.993354Z",
     "iopub.status.idle": "2025-05-16T17:21:17.335142Z",
     "shell.execute_reply": "2025-05-16T17:21:17.334124Z"
    },
    "papermill": {
     "duration": 6.348167,
     "end_time": "2025-05-16T17:21:17.336433",
     "exception": false,
     "start_time": "2025-05-16T17:21:10.988266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchtools\n",
      "  Downloading pytorchtools-0.0.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading pytorchtools-0.0.2-py2.py3-none-any.whl (3.1 kB)\n",
      "Installing collected packages: pytorchtools\n",
      "Successfully installed pytorchtools-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorchtools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581b86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:17.345519Z",
     "iopub.status.busy": "2025-05-16T17:21:17.345172Z",
     "iopub.status.idle": "2025-05-16T17:21:36.038130Z",
     "shell.execute_reply": "2025-05-16T17:21:36.037153Z"
    },
    "papermill": {
     "duration": 18.699217,
     "end_time": "2025-05-16T17:21:36.039528",
     "exception": false,
     "start_time": "2025-05-16T17:21:17.340311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thư viện hệ thống\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Xử lý XML và ảnh\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Torch và torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import (\n",
    "    box_convert, generalized_box_iou, generalized_box_iou_loss\n",
    ")\n",
    "\n",
    "# Thư viện augmentation và chuyển đổi tensor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Vẽ ảnh\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ee29b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.048493Z",
     "iopub.status.busy": "2025-05-16T17:21:36.047890Z",
     "iopub.status.idle": "2025-05-16T17:21:36.054735Z",
     "shell.execute_reply": "2025-05-16T17:21:36.053919Z"
    },
    "papermill": {
     "duration": 0.012474,
     "end_time": "2025-05-16T17:21:36.056018",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.043544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ca787e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.063992Z",
     "iopub.status.busy": "2025-05-16T17:21:36.063752Z",
     "iopub.status.idle": "2025-05-16T17:21:36.494283Z",
     "shell.execute_reply": "2025-05-16T17:21:36.493478Z"
    },
    "papermill": {
     "duration": 0.439609,
     "end_time": "2025-05-16T17:21:36.499194",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.059585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@58.066] global loadsave.cpp:241 findDecoder imread_('/kaggle/input/helo123/images/Abyssinian_1.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_81ng91bl78/croot/opencv-suite_1738943359148/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m annotation_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/helo123/annotations/xmls/Abyssinian_1.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Gọi hàm để vẽ ảnh với bounding box\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mplot_image_with_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/Abyssinian_1_bbox.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mplot_image_with_boxes\u001b[0;34m(image_path, annotation_path, save_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_image_with_boxes\u001b[39m(image_path, annotation_path, save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     objects \u001b[38;5;241m=\u001b[39m parse_annotation(annotation_path)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects:\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_81ng91bl78/croot/opencv-suite_1738943359148/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        box = {\n",
    "            'label': label,\n",
    "            'bbox': {\n",
    "                'xmin': int(bbox.find('xmin').text),\n",
    "                'ymin': int(bbox.find('ymin').text),\n",
    "                'xmax': int(bbox.find('xmax').text),\n",
    "                'ymax': int(bbox.find('ymax').text)\n",
    "            }\n",
    "        }\n",
    "        objects.append(box)\n",
    "    return objects\n",
    "\n",
    "def plot_image_with_boxes(image_path, annotation_path, save_path = None):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    objects = parse_annotation(annotation_path)\n",
    "\n",
    "    for obj in objects:\n",
    "        label = obj['label']\n",
    "        bbox = obj['bbox']\n",
    "        cv2.rectangle(image, \n",
    "                      (bbox['xmin'], bbox['ymin']), \n",
    "                      (bbox['xmax'], bbox['ymax']), \n",
    "                      color=(255, 0, 0), thickness=2)\n",
    "        cv2.putText(image, label, \n",
    "                    (bbox['xmin'], bbox['ymin'] - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, \n",
    "                    (0, 255, 0), 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if save_path:\n",
    "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(save_path, image_bgr)\n",
    "        print(f\"✅ Đã lưu ảnh có bounding box tại: {save_path}\")\n",
    "\n",
    "# Đường dẫn tuyệt đối đến file ảnh và file annotation\n",
    "image_path = '/kaggle/input/helo123/images/Abyssinian_1.jpg'\n",
    "annotation_path = '/kaggle/input/helo123/annotations/xmls/Abyssinian_1.xml'\n",
    "\n",
    "# Gọi hàm để vẽ ảnh với bounding box\n",
    "plot_image_with_boxes(image_path, annotation_path, save_path = '/kaggle/working/Abyssinian_1_bbox.jpg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab20e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.518140Z",
     "iopub.status.busy": "2025-05-16T17:21:36.517433Z",
     "iopub.status.idle": "2025-05-16T17:21:36.667684Z",
     "shell.execute_reply": "2025-05-16T17:21:36.666821Z"
    },
    "papermill": {
     "duration": 0.16008,
     "end_time": "2025-05-16T17:21:36.668854",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.508774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m backup_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working//no_annotation_images\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Tạo thư mục nếu chưa tồn tại\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackup_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Lấy danh sách tên file (không kèm đuôi)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(f\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Đường dẫn tới ảnh và annotation\n",
    "image_dir = '/kaggle/input/helo123/images'\n",
    "xml_dir = '/kaggle/input/helo123/annotations/xmls'\n",
    "backup_dir = '/kaggle/working//no_annotation_images'\n",
    "\n",
    "# Tạo thư mục nếu chưa tồn tại\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "# Lấy danh sách tên file (không kèm đuôi)\n",
    "image_ids = set(f.replace('.jpg', '') for f in os.listdir(image_dir) if f.endswith('.jpg'))\n",
    "xml_ids = set(f.replace('.xml', '') for f in os.listdir(xml_dir) if f.endswith('.xml'))\n",
    "\n",
    "# Tìm ảnh không có annotation\n",
    "no_xml_ids = image_ids - xml_ids\n",
    "print(f\"📦 Số ảnh không có annotation: {len(no_xml_ids)}\")\n",
    "\n",
    "# Di chuyển từng ảnh vào thư mục backup\n",
    "for img_id in no_xml_ids:\n",
    "    src = os.path.join(image_dir, img_id + '.jpg')\n",
    "    dst = os.path.join(backup_dir, img_id + '.jpg')\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "print(f\"✅ Đã di chuyển {len(no_xml_ids)} ảnh sang thư mục: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a986430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.686627Z",
     "iopub.status.busy": "2025-05-16T17:21:36.686375Z",
     "iopub.status.idle": "2025-05-16T17:21:36.691041Z",
     "shell.execute_reply": "2025-05-16T17:21:36.690480Z"
    },
    "papermill": {
     "duration": 0.014471,
     "end_time": "2025-05-16T17:21:36.691991",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.677520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "def random_split(ids, train_ratio, val_ratio, test_ratio):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Tỉ lệ chia không hợp lệ!\"\n",
    "    \n",
    "    random.shuffle(ids)\n",
    "    total_size = len(ids)\n",
    "    \n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_ids = ids[:train_size]\n",
    "    val_ids = ids[train_size:train_size + val_size]\n",
    "    test_ids = ids[train_size + val_size:]\n",
    "\n",
    "    return train_ids, val_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638fe5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.709341Z",
     "iopub.status.busy": "2025-05-16T17:21:36.708652Z",
     "iopub.status.idle": "2025-05-16T17:21:36.715997Z",
     "shell.execute_reply": "2025-05-16T17:21:36.715489Z"
    },
    "papermill": {
     "duration": 0.016971,
     "end_time": "2025-05-16T17:21:36.717076",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.700105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, image_ids, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        xml_name = img_name.replace('.jpg', '.xml')\n",
    "        xml_path = os.path.join(self.xml_dir, xml_name)\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        obj = root.find('object')\n",
    "        label = 0 if obj.find('name').text.lower() == 'cat' else 1\n",
    "\n",
    "        bndbox = obj.find('bndbox')\n",
    "        box = [\n",
    "            float(bndbox.find('xmin').text),\n",
    "            float(bndbox.find('ymin').text),\n",
    "            float(bndbox.find('xmax').text),\n",
    "            float(bndbox.find('ymax').text)\n",
    "        ]\n",
    "\n",
    "        if self.transform:\n",
    "            img_np = np.array(img)\n",
    "            transformed = self.transform(image=img_np, bboxes=[box], labels=[label])\n",
    "            img = transformed['image']\n",
    "            box = transformed['bboxes'][0]\n",
    "            label = transformed['labels'][0]\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.long), torch.tensor(box, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e129acee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.733408Z",
     "iopub.status.busy": "2025-05-16T17:21:36.733213Z",
     "iopub.status.idle": "2025-05-16T17:21:37.924036Z",
     "shell.execute_reply": "2025-05-16T17:21:37.922945Z"
    },
    "papermill": {
     "duration": 1.200676,
     "end_time": "2025-05-16T17:21:37.925623",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.724947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/helo123/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m xml_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/helo123/annotations/xmls\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Lấy danh sách file ảnh\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m image_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Chia train, val, test\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_ids, val_ids, test_ids \u001b[38;5;241m=\u001b[39m random_split(image_ids, train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, test_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/helo123/images'"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3), \n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.1, rotate_limit=10, p=0.3, border_mode=0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "\n",
    "val_test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),  # Cần chuẩn hóa ảnh trước khi vào model pretrain\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "# Đường dẫn dữ liệu\n",
    "image_dir = '/kaggle/input/helo123/images'\n",
    "xml_dir = '/kaggle/input/helo123/annotations/xmls'\n",
    "\n",
    "# Lấy danh sách file ảnh\n",
    "image_ids = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "\n",
    "# Chia train, val, test\n",
    "train_ids, val_ids, test_ids = random_split(image_ids, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "# Khởi tạo Dataset\n",
    "train_dataset = PetDataset(image_dir, xml_dir, train_ids, transform=train_transform)\n",
    "val_dataset   = PetDataset(image_dir, xml_dir, val_ids, transform=val_test_transform)\n",
    "test_dataset  = PetDataset(image_dir, xml_dir, test_ids, transform=val_test_transform)\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Kiểm tra\n",
    "for images, labels, bboxes in train_loader:\n",
    "    print(\"Train batch - Images:\", images.shape, \"Labels:\", labels.shape, \"BBoxes:\", bboxes.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f3e953e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:37.945692Z",
     "iopub.status.busy": "2025-05-16T17:21:37.945395Z",
     "iopub.status.idle": "2025-05-16T17:21:37.953216Z",
     "shell.execute_reply": "2025-05-16T17:21:37.952678Z"
    },
    "papermill": {
     "duration": 0.018635,
     "end_time": "2025-05-16T17:21:37.954238",
     "exception": false,
     "start_time": "2025-05-16T17:21:37.935603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNetDetector(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_stem=True):\n",
    "        super(ResNetDetector, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Phần stem: conv1 → bn1 → relu → maxpool\n",
    "        self.stem = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "        )\n",
    "        \n",
    "        # Luôn train layer1–layer4\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "        if freeze_stem:\n",
    "            for param in self.stem.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Pooling & heads\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        self.bbox_regressor = nn.Linear(2048, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)           # [B,  64, H/4, W/4]\n",
    "        x = self.layer1(x)         # [B, 256, H/4, W/4]\n",
    "        x = self.layer2(x)         # [B, 512, H/8, W/8]\n",
    "        x = self.layer3(x)         # [B,1024, H/16,W/16]\n",
    "        x = self.layer4(x)         # [B,2048, H/32,W/32]\n",
    "        x = self.avgpool(x)        # [B,2048, 1, 1]\n",
    "        flat = self.flatten(x)     # [B,2048]\n",
    "\n",
    "        cls_logits = self.classifier(flat)\n",
    "        bbox_pred  = self.bbox_regressor(flat)\n",
    "        return bbox_pred, cls_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba590523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:37.971368Z",
     "iopub.status.busy": "2025-05-16T17:21:37.971153Z",
     "iopub.status.idle": "2025-05-16T17:21:39.564632Z",
     "shell.execute_reply": "2025-05-16T17:21:39.563821Z"
    },
    "papermill": {
     "duration": 1.603671,
     "end_time": "2025-05-16T17:21:39.566060",
     "exception": false,
     "start_time": "2025-05-16T17:21:37.962389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/minhkha/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:32<00:00, 3.17MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNetDetector(num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='/kaggle/working/resnet100_earlystop_best.pth') \n",
    "num_epochs = 100\n",
    "\n",
    "# Loss functions\n",
    "criterion_cls = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_bbox = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "def compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes):\n",
    "    pred_boxes_xyxy = box_convert(pred_boxes, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "    gt_boxes_xyxy = box_convert(gt_boxes, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "    ious = generalized_box_iou(pred_boxes_xyxy, gt_boxes_xyxy).diag()\n",
    "\n",
    "    batch_size = pred_cls.size(0)\n",
    "    num_classes = pred_cls.size(1)\n",
    "    gt_onehot = torch.zeros((batch_size, num_classes), device=pred_cls.device)\n",
    "    gt_onehot.scatter_(1, gt_labels.unsqueeze(1), 1.0)\n",
    "\n",
    "    soft_labels = gt_onehot * ious.unsqueeze(1)\n",
    "    cls_loss = criterion_cls(pred_cls, soft_labels)\n",
    "    bbox_loss = criterion_bbox(pred_boxes, gt_boxes)\n",
    "\n",
    "    return cls_loss + bbox_loss\n",
    "\n",
    "# # --- Training loop ---\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_path = 'resnet50_100epoch.pth'\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # --- Training ---\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, gt_labels, gt_boxes in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         gt_labels = gt_labels.to(device)\n",
    "#         gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "#         pred_boxes, pred_cls = model(images)\n",
    "#         loss = compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     train_losses.append(epoch_loss)\n",
    "\n",
    "#     # --- Validation ---\n",
    "#     model.eval()\n",
    "#     val_running_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for images, gt_labels, gt_boxes in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             gt_labels = gt_labels.to(device)\n",
    "#             gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "#             pred_boxes, pred_cls = model(images)\n",
    "#             val_loss = compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes)\n",
    "\n",
    "#             val_running_loss += val_loss.item() * images.size(0)\n",
    "\n",
    "#     val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "#     val_losses.append(val_epoch_loss)\n",
    "\n",
    "#  # --- Print log ---\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- EarlyStopping check ---\n",
    "#     early_stopping(val_epoch_loss, model)\n",
    "\n",
    "#     # --- Save losses ---\n",
    "#     torch.save({\n",
    "#         'train_losses': train_losses,\n",
    "#         'val_losses': val_losses\n",
    "#     }, 'losses.pt')\n",
    "\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"=> Early stopping triggered.\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43bba7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:39.585599Z",
     "iopub.status.busy": "2025-05-16T17:21:39.585108Z",
     "iopub.status.idle": "2025-05-16T17:21:41.118581Z",
     "shell.execute_reply": "2025-05-16T17:21:41.117969Z"
    },
    "papermill": {
     "duration": 1.544416,
     "end_time": "2025-05-16T17:21:41.119930",
     "exception": false,
     "start_time": "2025-05-16T17:21:39.575514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Tính IoU giữa hai hộp giới hạn.\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "def evaluate_model(model, dataloader, compute_bounding_box_loss, generalized_box_iou, iou_threshold=0.5, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    correct_localization = 0\n",
    "    correct_both = 0\n",
    "    total_iou = 0\n",
    "    total_giou = 0\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_labels, gt_boxes in dataloader:\n",
    "            images = images.to(device)\n",
    "            gt_labels = gt_labels.to(device)\n",
    "            gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "            pred_boxes, pred_cls = model(images)\n",
    "            loss = compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            pred_labels = pred_cls.argmax(dim=1)\n",
    "\n",
    "            for pred_box, pred_label, gt_box, gt_label in zip(pred_boxes, pred_labels, gt_boxes, gt_labels):\n",
    "                pred_box = pred_box.cpu()\n",
    "                gt_box = gt_box.cpu()\n",
    "                pred_label = pred_label.cpu()\n",
    "                gt_label = gt_label.cpu()\n",
    "\n",
    "                iou = compute_iou(pred_box, gt_box)\n",
    "\n",
    "                total_iou += iou\n",
    "\n",
    "                if iou >= iou_threshold:\n",
    "                    correct_localization += 1\n",
    "                if pred_label == gt_label and iou >= iou_threshold:\n",
    "                    correct_both += 1\n",
    "\n",
    "                tp = int(pred_label == gt_label and iou >= iou_threshold)\n",
    "                fp = int(pred_label != gt_label)\n",
    "                fn = int(iou < iou_threshold)\n",
    "\n",
    "                precision = tp / (tp + fp + 1e-6)\n",
    "                recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "\n",
    "                total_samples += 1\n",
    "\n",
    "                y_true.append(gt_label.item())\n",
    "                y_pred.append(pred_label.item())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    localization_accuracy = correct_localization / total_samples\n",
    "    combined_accuracy = correct_both / total_samples\n",
    "    mean_iou = total_iou / total_samples\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "\n",
    "    # Tạo báo cáo phân loại chi tiết\n",
    "    report = classification_report(y_true, y_pred, digits=4, output_dict=True, zero_division=0)\n",
    "\n",
    "    # In kết quả\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"- Độ chính xác định vị (IoU >= {iou_threshold}): {localization_accuracy:.4f}\")\n",
    "    print(f\"- Độ chính xác kết hợp: {combined_accuracy:.4f}\")\n",
    "    print(f\"- Trung bình IoU: {mean_iou:.4f}\")\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"loc_acc\": localization_accuracy,\n",
    "        \"combined_acc\": combined_accuracy,\n",
    "        \"mean_iou\": mean_iou,\n",
    "        \"classification_report\": report,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c451533f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:41.138022Z",
     "iopub.status.busy": "2025-05-16T17:21:41.137543Z",
     "iopub.status.idle": "2025-05-16T17:21:52.228368Z",
     "shell.execute_reply": "2025-05-16T17:21:52.227400Z"
    },
    "papermill": {
     "duration": 11.108818,
     "end_time": "2025-05-16T17:21:52.237586",
     "exception": false,
     "start_time": "2025-05-16T17:21:41.128768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Hàm trực quan hóa dự đoán\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNetDetector(num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='/kaggle/working/resnet100_earlystop_best.pth') \n",
    "\n",
    "# Loss functions\n",
    "criterion_cls = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_bbox = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "def visualize_predictions(model, dataset, idx=0, device=\"cuda\", mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Hiển thị ảnh với hộp dự đoán và hộp thực tế.\"\"\"\n",
    "    model.eval()\n",
    "    img, gt_label, gt_box = dataset[idx]\n",
    "    img_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_box, pred_cls = model(img_tensor)\n",
    "        pred_label = pred_cls[0].argmax().item()\n",
    "        pred_box = pred_box[0].cpu()\n",
    "\n",
    "    # Đảo ngược chuẩn hóa ảnh\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array(std) + np.array(mean)\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.imshow(img_np)\n",
    "\n",
    "    # Vẽ hộp thực tế (ground-truth)\n",
    "    rect = patches.Rectangle((gt_box[0], gt_box[1]), gt_box[2]-gt_box[0], gt_box[3]-gt_box[1],\n",
    "                             linewidth=2, edgecolor='green', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(gt_box[0], gt_box[1] - 4, f\"GT: {gt_label.item()}\", color='green')\n",
    "\n",
    "    # Vẽ hộp dự đoán\n",
    "    rect = patches.Rectangle((pred_box[0], pred_box[1]), pred_box[2]-pred_box[0], pred_box[3]-pred_box[1],\n",
    "                             linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(pred_box[0], pred_box[1] + 4, f\"Pred: {pred_label}\", color='red')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Load mô hình tốt nhất trước khi đánh giá\n",
    "# model.load_state_dict(torch.load('/kaggle/input/hehe123/resnet100_earlystop_best.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # Đánh giá và trực quan hóa trên tập test\n",
    "# print(\"\\nĐánh giá trên tập test:\")\n",
    "# test_loss= evaluate_model(\n",
    "#     model, test_loader,  compute_bounding_box_loss,generalized_box_iou=generalized_box_iou, iou_threshold=0.5, device=device\n",
    "# )\n",
    "\n",
    "# print(\"Trực quan hóa một số mẫu từ tập test:\")\n",
    "# for idx in range(5):\n",
    "#     if idx < len(test_dataset):\n",
    "#         print(f\"Mẫu test {idx}:\")\n",
    "#         visualize_predictions(model, test_dataset, idx=idx, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16938e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:52.395231Z",
     "iopub.status.busy": "2025-05-16T17:21:52.394924Z",
     "iopub.status.idle": "2025-05-16T17:21:52.534588Z",
     "shell.execute_reply": "2025-05-16T17:21:52.533495Z"
    },
    "papermill": {
     "duration": 0.217346,
     "end_time": "2025-05-16T17:21:52.535784",
     "exception": true,
     "start_time": "2025-05-16T17:21:52.318438",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/1326798179.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Vẽ đồ thị loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training vs Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d5e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a388ca0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:16:26.867273Z",
     "iopub.status.busy": "2025-05-16T17:16:26.866602Z",
     "iopub.status.idle": "2025-05-16T17:16:27.429116Z",
     "shell.execute_reply": "2025-05-16T17:16:27.428370Z",
     "shell.execute_reply.started": "2025-05-16T17:16:26.867249Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ======= CẤU HÌNH =======\n",
    "input_dir = \"/Users/minhkha/Downloads/images-2.jpeg\"\n",
    "output_dir = \"/Users/minhkha/Desktop/minh_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ======= THÔNG SỐ CHUẨN HÓA (nếu dùng ImageNet) =======\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ======= LOAD MÔ HÌNH =======\n",
    "model = ResNetDetector()\n",
    "model.load_state_dict(torch.load('/Users/minhkha/Downloads/cs231-official.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ======= TIỀN XỬ LÝ ẢNH =======\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# ======= LẶP QUA CÁC ẢNH =======\n",
    "# for img_file in os.listdir(input_dir):\n",
    "#     if not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#         continue\n",
    "\n",
    "img_path = input_dir\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "image_resized = image.resize((224, 224))  # để vẽ lại\n",
    "\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        pred_box, pred_cls = model(input_tensor)\n",
    "        pred_label = pred_cls[0].argmax().item()\n",
    "        pred_box = pred_box[0].cpu().numpy()  # [x1, y1, x2, y2]\n",
    "\n",
    "    # VẼ KẾT QUẢ\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(image_resized)\n",
    "\n",
    "rect = patches.Rectangle(\n",
    "        (pred_box[0], pred_box[1]),\n",
    "        pred_box[2] - pred_box[0],\n",
    "        pred_box[3] - pred_box[1],\n",
    "        linewidth=2, edgecolor='red', facecolor='none'\n",
    "    )\n",
    "ax.add_patch(rect)\n",
    "ax.text(pred_box[0], pred_box[1] + 4, f\"Pred: {pred_label}\", color='green')\n",
    "ax.axis(\"off\")\n",
    "img_name = os.path.basename(img_path)\n",
    "output_path = os.path.join(output_dir, img_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.title(f\"Pred: {pred_label}\")\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7122831,
     "sourceId": 11376892,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7438200,
     "sourceId": 11838935,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 51.397918,
   "end_time": "2025-05-16T17:21:55.683153",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-16T17:21:04.285235",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
