{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12658ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:10.994073Z",
     "iopub.status.busy": "2025-05-16T17:21:10.993354Z",
     "iopub.status.idle": "2025-05-16T17:21:17.335142Z",
     "shell.execute_reply": "2025-05-16T17:21:17.334124Z"
    },
    "papermill": {
     "duration": 6.348167,
     "end_time": "2025-05-16T17:21:17.336433",
     "exception": false,
     "start_time": "2025-05-16T17:21:10.988266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchtools\n",
      "  Downloading pytorchtools-0.0.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading pytorchtools-0.0.2-py2.py3-none-any.whl (3.1 kB)\n",
      "Installing collected packages: pytorchtools\n",
      "Successfully installed pytorchtools-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorchtools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581b86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:17.345519Z",
     "iopub.status.busy": "2025-05-16T17:21:17.345172Z",
     "iopub.status.idle": "2025-05-16T17:21:36.038130Z",
     "shell.execute_reply": "2025-05-16T17:21:36.037153Z"
    },
    "papermill": {
     "duration": 18.699217,
     "end_time": "2025-05-16T17:21:36.039528",
     "exception": false,
     "start_time": "2025-05-16T17:21:17.340311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Th∆∞ vi·ªán h·ªá th·ªëng\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# X·ª≠ l√Ω XML v√† ·∫£nh\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Torch v√† torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import (\n",
    "    box_convert, generalized_box_iou, generalized_box_iou_loss\n",
    ")\n",
    "\n",
    "# Th∆∞ vi·ªán augmentation v√† chuy·ªÉn ƒë·ªïi tensor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# V·∫Ω ·∫£nh\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ee29b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.048493Z",
     "iopub.status.busy": "2025-05-16T17:21:36.047890Z",
     "iopub.status.idle": "2025-05-16T17:21:36.054735Z",
     "shell.execute_reply": "2025-05-16T17:21:36.053919Z"
    },
    "papermill": {
     "duration": 0.012474,
     "end_time": "2025-05-16T17:21:36.056018",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.043544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} ‚Üí {val_loss:.6f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ca787e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.063992Z",
     "iopub.status.busy": "2025-05-16T17:21:36.063752Z",
     "iopub.status.idle": "2025-05-16T17:21:36.494283Z",
     "shell.execute_reply": "2025-05-16T17:21:36.493478Z"
    },
    "papermill": {
     "duration": 0.439609,
     "end_time": "2025-05-16T17:21:36.499194",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.059585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@58.066] global loadsave.cpp:241 findDecoder imread_('/kaggle/input/helo123/images/Abyssinian_1.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_81ng91bl78/croot/opencv-suite_1738943359148/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m annotation_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/helo123/annotations/xmls/Abyssinian_1.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# G·ªçi h√†m ƒë·ªÉ v·∫Ω ·∫£nh v·ªõi bounding box\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mplot_image_with_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/Abyssinian_1_bbox.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mplot_image_with_boxes\u001b[0;34m(image_path, annotation_path, save_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_image_with_boxes\u001b[39m(image_path, annotation_path, save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     objects \u001b[38;5;241m=\u001b[39m parse_annotation(annotation_path)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects:\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_81ng91bl78/croot/opencv-suite_1738943359148/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        box = {\n",
    "            'label': label,\n",
    "            'bbox': {\n",
    "                'xmin': int(bbox.find('xmin').text),\n",
    "                'ymin': int(bbox.find('ymin').text),\n",
    "                'xmax': int(bbox.find('xmax').text),\n",
    "                'ymax': int(bbox.find('ymax').text)\n",
    "            }\n",
    "        }\n",
    "        objects.append(box)\n",
    "    return objects\n",
    "\n",
    "def plot_image_with_boxes(image_path, annotation_path, save_path = None):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    objects = parse_annotation(annotation_path)\n",
    "\n",
    "    for obj in objects:\n",
    "        label = obj['label']\n",
    "        bbox = obj['bbox']\n",
    "        cv2.rectangle(image, \n",
    "                      (bbox['xmin'], bbox['ymin']), \n",
    "                      (bbox['xmax'], bbox['ymax']), \n",
    "                      color=(255, 0, 0), thickness=2)\n",
    "        cv2.putText(image, label, \n",
    "                    (bbox['xmin'], bbox['ymin'] - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, \n",
    "                    (0, 255, 0), 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if save_path:\n",
    "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(save_path, image_bgr)\n",
    "        print(f\"‚úÖ ƒê√£ l∆∞u ·∫£nh c√≥ bounding box t·∫°i: {save_path}\")\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn file ·∫£nh v√† file annotation\n",
    "image_path = '/kaggle/input/helo123/images/Abyssinian_1.jpg'\n",
    "annotation_path = '/kaggle/input/helo123/annotations/xmls/Abyssinian_1.xml'\n",
    "\n",
    "# G·ªçi h√†m ƒë·ªÉ v·∫Ω ·∫£nh v·ªõi bounding box\n",
    "plot_image_with_boxes(image_path, annotation_path, save_path = '/kaggle/working/Abyssinian_1_bbox.jpg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab20e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.518140Z",
     "iopub.status.busy": "2025-05-16T17:21:36.517433Z",
     "iopub.status.idle": "2025-05-16T17:21:36.667684Z",
     "shell.execute_reply": "2025-05-16T17:21:36.666821Z"
    },
    "papermill": {
     "duration": 0.16008,
     "end_time": "2025-05-16T17:21:36.668854",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.508774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m backup_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working//no_annotation_images\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackup_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# L·∫•y danh s√°ch t√™n file (kh√¥ng k√®m ƒëu√¥i)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(f\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi ·∫£nh v√† annotation\n",
    "image_dir = '/kaggle/input/helo123/images'\n",
    "xml_dir = '/kaggle/input/helo123/annotations/xmls'\n",
    "backup_dir = '/kaggle/working//no_annotation_images'\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "# L·∫•y danh s√°ch t√™n file (kh√¥ng k√®m ƒëu√¥i)\n",
    "image_ids = set(f.replace('.jpg', '') for f in os.listdir(image_dir) if f.endswith('.jpg'))\n",
    "xml_ids = set(f.replace('.xml', '') for f in os.listdir(xml_dir) if f.endswith('.xml'))\n",
    "\n",
    "# T√¨m ·∫£nh kh√¥ng c√≥ annotation\n",
    "no_xml_ids = image_ids - xml_ids\n",
    "print(f\"üì¶ S·ªë ·∫£nh kh√¥ng c√≥ annotation: {len(no_xml_ids)}\")\n",
    "\n",
    "# Di chuy·ªÉn t·ª´ng ·∫£nh v√†o th∆∞ m·ª•c backup\n",
    "for img_id in no_xml_ids:\n",
    "    src = os.path.join(image_dir, img_id + '.jpg')\n",
    "    dst = os.path.join(backup_dir, img_id + '.jpg')\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ di chuy·ªÉn {len(no_xml_ids)} ·∫£nh sang th∆∞ m·ª•c: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a986430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.686627Z",
     "iopub.status.busy": "2025-05-16T17:21:36.686375Z",
     "iopub.status.idle": "2025-05-16T17:21:36.691041Z",
     "shell.execute_reply": "2025-05-16T17:21:36.690480Z"
    },
    "papermill": {
     "duration": 0.014471,
     "end_time": "2025-05-16T17:21:36.691991",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.677520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "def random_split(ids, train_ratio, val_ratio, test_ratio):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"T·ªâ l·ªá chia kh√¥ng h·ª£p l·ªá!\"\n",
    "    \n",
    "    random.shuffle(ids)\n",
    "    total_size = len(ids)\n",
    "    \n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_ids = ids[:train_size]\n",
    "    val_ids = ids[train_size:train_size + val_size]\n",
    "    test_ids = ids[train_size + val_size:]\n",
    "\n",
    "    return train_ids, val_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638fe5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.709341Z",
     "iopub.status.busy": "2025-05-16T17:21:36.708652Z",
     "iopub.status.idle": "2025-05-16T17:21:36.715997Z",
     "shell.execute_reply": "2025-05-16T17:21:36.715489Z"
    },
    "papermill": {
     "duration": 0.016971,
     "end_time": "2025-05-16T17:21:36.717076",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.700105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, image_ids, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        xml_name = img_name.replace('.jpg', '.xml')\n",
    "        xml_path = os.path.join(self.xml_dir, xml_name)\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        obj = root.find('object')\n",
    "        label = 0 if obj.find('name').text.lower() == 'cat' else 1\n",
    "\n",
    "        bndbox = obj.find('bndbox')\n",
    "        box = [\n",
    "            float(bndbox.find('xmin').text),\n",
    "            float(bndbox.find('ymin').text),\n",
    "            float(bndbox.find('xmax').text),\n",
    "            float(bndbox.find('ymax').text)\n",
    "        ]\n",
    "\n",
    "        if self.transform:\n",
    "            img_np = np.array(img)\n",
    "            transformed = self.transform(image=img_np, bboxes=[box], labels=[label])\n",
    "            img = transformed['image']\n",
    "            box = transformed['bboxes'][0]\n",
    "            label = transformed['labels'][0]\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.long), torch.tensor(box, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e129acee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:36.733408Z",
     "iopub.status.busy": "2025-05-16T17:21:36.733213Z",
     "iopub.status.idle": "2025-05-16T17:21:37.924036Z",
     "shell.execute_reply": "2025-05-16T17:21:37.922945Z"
    },
    "papermill": {
     "duration": 1.200676,
     "end_time": "2025-05-16T17:21:37.925623",
     "exception": false,
     "start_time": "2025-05-16T17:21:36.724947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/helo123/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m xml_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/helo123/annotations/xmls\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# L·∫•y danh s√°ch file ·∫£nh\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m image_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Chia train, val, test\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_ids, val_ids, test_ids \u001b[38;5;241m=\u001b[39m random_split(image_ids, train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, test_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/helo123/images'"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3), \n",
    "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.1, rotate_limit=10, p=0.3, border_mode=0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "\n",
    "val_test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),  # C·∫ßn chu·∫©n h√≥a ·∫£nh tr∆∞·ªõc khi v√†o model pretrain\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "# ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
    "image_dir = '/kaggle/input/helo123/images'\n",
    "xml_dir = '/kaggle/input/helo123/annotations/xmls'\n",
    "\n",
    "# L·∫•y danh s√°ch file ·∫£nh\n",
    "image_ids = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "\n",
    "# Chia train, val, test\n",
    "train_ids, val_ids, test_ids = random_split(image_ids, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "# Kh·ªüi t·∫°o Dataset\n",
    "train_dataset = PetDataset(image_dir, xml_dir, train_ids, transform=train_transform)\n",
    "val_dataset   = PetDataset(image_dir, xml_dir, val_ids, transform=val_test_transform)\n",
    "test_dataset  = PetDataset(image_dir, xml_dir, test_ids, transform=val_test_transform)\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Ki·ªÉm tra\n",
    "for images, labels, bboxes in train_loader:\n",
    "    print(\"Train batch - Images:\", images.shape, \"Labels:\", labels.shape, \"BBoxes:\", bboxes.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f3e953e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:37.945692Z",
     "iopub.status.busy": "2025-05-16T17:21:37.945395Z",
     "iopub.status.idle": "2025-05-16T17:21:37.953216Z",
     "shell.execute_reply": "2025-05-16T17:21:37.952678Z"
    },
    "papermill": {
     "duration": 0.018635,
     "end_time": "2025-05-16T17:21:37.954238",
     "exception": false,
     "start_time": "2025-05-16T17:21:37.935603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNetDetector(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_stem=True):\n",
    "        super(ResNetDetector, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Ph·∫ßn stem: conv1 ‚Üí bn1 ‚Üí relu ‚Üí maxpool\n",
    "        self.stem = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "        )\n",
    "        \n",
    "        # Lu√¥n train layer1‚Äìlayer4\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "        if freeze_stem:\n",
    "            for param in self.stem.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Pooling & heads\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        self.bbox_regressor = nn.Linear(2048, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)           # [B,  64, H/4, W/4]\n",
    "        x = self.layer1(x)         # [B, 256, H/4, W/4]\n",
    "        x = self.layer2(x)         # [B, 512, H/8, W/8]\n",
    "        x = self.layer3(x)         # [B,1024, H/16,W/16]\n",
    "        x = self.layer4(x)         # [B,2048, H/32,W/32]\n",
    "        x = self.avgpool(x)        # [B,2048, 1, 1]\n",
    "        flat = self.flatten(x)     # [B,2048]\n",
    "\n",
    "        cls_logits = self.classifier(flat)\n",
    "        bbox_pred  = self.bbox_regressor(flat)\n",
    "        return bbox_pred, cls_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba590523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:37.971368Z",
     "iopub.status.busy": "2025-05-16T17:21:37.971153Z",
     "iopub.status.idle": "2025-05-16T17:21:39.564632Z",
     "shell.execute_reply": "2025-05-16T17:21:39.563821Z"
    },
    "papermill": {
     "duration": 1.603671,
     "end_time": "2025-05-16T17:21:39.566060",
     "exception": false,
     "start_time": "2025-05-16T17:21:37.962389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/minhkha/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:32<00:00, 3.17MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import box_convert, generalized_box_iou\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNetDetector(num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='/kaggle/working/resnet100_earlystop_best.pth') \n",
    "num_epochs = 100\n",
    "\n",
    "# Loss functions\n",
    "criterion_cls = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_bbox = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "def compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes):\n",
    "    pred_boxes_xyxy = box_convert(pred_boxes, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "    gt_boxes_xyxy = box_convert(gt_boxes, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "    ious = generalized_box_iou(pred_boxes_xyxy, gt_boxes_xyxy).diag()\n",
    "\n",
    "    batch_size = pred_cls.size(0)\n",
    "    num_classes = pred_cls.size(1)\n",
    "    gt_onehot = torch.zeros((batch_size, num_classes), device=pred_cls.device)\n",
    "    gt_onehot.scatter_(1, gt_labels.unsqueeze(1), 1.0)\n",
    "\n",
    "    soft_labels = gt_onehot * ious.unsqueeze(1)\n",
    "    cls_loss = criterion_cls(pred_cls, soft_labels)\n",
    "    bbox_loss = criterion_bbox(pred_boxes, gt_boxes)\n",
    "\n",
    "    return cls_loss + bbox_loss\n",
    "\n",
    "# # --- Training loop ---\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_path = 'resnet50_100epoch.pth'\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # --- Training ---\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, gt_labels, gt_boxes in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         gt_labels = gt_labels.to(device)\n",
    "#         gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "#         pred_boxes, pred_cls = model(images)\n",
    "#         loss = compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     train_losses.append(epoch_loss)\n",
    "\n",
    "#     # --- Validation ---\n",
    "#     model.eval()\n",
    "#     val_running_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for images, gt_labels, gt_boxes in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             gt_labels = gt_labels.to(device)\n",
    "#             gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "#             pred_boxes, pred_cls = model(images)\n",
    "#             val_loss = compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes)\n",
    "\n",
    "#             val_running_loss += val_loss.item() * images.size(0)\n",
    "\n",
    "#     val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "#     val_losses.append(val_epoch_loss)\n",
    "\n",
    "#  # --- Print log ---\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "#     # --- EarlyStopping check ---\n",
    "#     early_stopping(val_epoch_loss, model)\n",
    "\n",
    "#     # --- Save losses ---\n",
    "#     torch.save({\n",
    "#         'train_losses': train_losses,\n",
    "#         'val_losses': val_losses\n",
    "#     }, 'losses.pt')\n",
    "\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"=> Early stopping triggered.\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43bba7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:39.585599Z",
     "iopub.status.busy": "2025-05-16T17:21:39.585108Z",
     "iopub.status.idle": "2025-05-16T17:21:41.118581Z",
     "shell.execute_reply": "2025-05-16T17:21:41.117969Z"
    },
    "papermill": {
     "duration": 1.544416,
     "end_time": "2025-05-16T17:21:41.119930",
     "exception": false,
     "start_time": "2025-05-16T17:21:39.575514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"T√≠nh IoU gi·ªØa hai h·ªôp gi·ªõi h·∫°n.\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "def evaluate_model(model, dataloader, compute_bounding_box_loss, generalized_box_iou, iou_threshold=0.5, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    correct_localization = 0\n",
    "    correct_both = 0\n",
    "    total_iou = 0\n",
    "    total_giou = 0\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_labels, gt_boxes in dataloader:\n",
    "            images = images.to(device)\n",
    "            gt_labels = gt_labels.to(device)\n",
    "            gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "            pred_boxes, pred_cls = model(images)\n",
    "            loss = compute_bounding_box_loss(pred_cls, pred_boxes, gt_labels, gt_boxes)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            pred_labels = pred_cls.argmax(dim=1)\n",
    "\n",
    "            for pred_box, pred_label, gt_box, gt_label in zip(pred_boxes, pred_labels, gt_boxes, gt_labels):\n",
    "                pred_box = pred_box.cpu()\n",
    "                gt_box = gt_box.cpu()\n",
    "                pred_label = pred_label.cpu()\n",
    "                gt_label = gt_label.cpu()\n",
    "\n",
    "                iou = compute_iou(pred_box, gt_box)\n",
    "\n",
    "                total_iou += iou\n",
    "\n",
    "                if iou >= iou_threshold:\n",
    "                    correct_localization += 1\n",
    "                if pred_label == gt_label and iou >= iou_threshold:\n",
    "                    correct_both += 1\n",
    "\n",
    "                tp = int(pred_label == gt_label and iou >= iou_threshold)\n",
    "                fp = int(pred_label != gt_label)\n",
    "                fn = int(iou < iou_threshold)\n",
    "\n",
    "                precision = tp / (tp + fp + 1e-6)\n",
    "                recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "\n",
    "                total_samples += 1\n",
    "\n",
    "                y_true.append(gt_label.item())\n",
    "                y_pred.append(pred_label.item())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    localization_accuracy = correct_localization / total_samples\n",
    "    combined_accuracy = correct_both / total_samples\n",
    "    mean_iou = total_iou / total_samples\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "\n",
    "    # T·∫°o b√°o c√°o ph√¢n lo·∫°i chi ti·∫øt\n",
    "    report = classification_report(y_true, y_pred, digits=4, output_dict=True, zero_division=0)\n",
    "\n",
    "    # In k·∫øt qu·∫£\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"- ƒê·ªô ch√≠nh x√°c ƒë·ªãnh v·ªã (IoU >= {iou_threshold}): {localization_accuracy:.4f}\")\n",
    "    print(f\"- ƒê·ªô ch√≠nh x√°c k·∫øt h·ª£p: {combined_accuracy:.4f}\")\n",
    "    print(f\"- Trung b√¨nh IoU: {mean_iou:.4f}\")\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"loc_acc\": localization_accuracy,\n",
    "        \"combined_acc\": combined_accuracy,\n",
    "        \"mean_iou\": mean_iou,\n",
    "        \"classification_report\": report,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c451533f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:41.138022Z",
     "iopub.status.busy": "2025-05-16T17:21:41.137543Z",
     "iopub.status.idle": "2025-05-16T17:21:52.228368Z",
     "shell.execute_reply": "2025-05-16T17:21:52.227400Z"
    },
    "papermill": {
     "duration": 11.108818,
     "end_time": "2025-05-16T17:21:52.237586",
     "exception": false,
     "start_time": "2025-05-16T17:21:41.128768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# H√†m tr·ª±c quan h√≥a d·ª± ƒëo√°n\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNetDetector(num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='/kaggle/working/resnet100_earlystop_best.pth') \n",
    "\n",
    "# Loss functions\n",
    "criterion_cls = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_bbox = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "def visualize_predictions(model, dataset, idx=0, device=\"cuda\", mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Hi·ªÉn th·ªã ·∫£nh v·ªõi h·ªôp d·ª± ƒëo√°n v√† h·ªôp th·ª±c t·∫ø.\"\"\"\n",
    "    model.eval()\n",
    "    img, gt_label, gt_box = dataset[idx]\n",
    "    img_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_box, pred_cls = model(img_tensor)\n",
    "        pred_label = pred_cls[0].argmax().item()\n",
    "        pred_box = pred_box[0].cpu()\n",
    "\n",
    "    # ƒê·∫£o ng∆∞·ª£c chu·∫©n h√≥a ·∫£nh\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array(std) + np.array(mean)\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.imshow(img_np)\n",
    "\n",
    "    # V·∫Ω h·ªôp th·ª±c t·∫ø (ground-truth)\n",
    "    rect = patches.Rectangle((gt_box[0], gt_box[1]), gt_box[2]-gt_box[0], gt_box[3]-gt_box[1],\n",
    "                             linewidth=2, edgecolor='green', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(gt_box[0], gt_box[1] - 4, f\"GT: {gt_label.item()}\", color='green')\n",
    "\n",
    "    # V·∫Ω h·ªôp d·ª± ƒëo√°n\n",
    "    rect = patches.Rectangle((pred_box[0], pred_box[1]), pred_box[2]-pred_box[0], pred_box[3]-pred_box[1],\n",
    "                             linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(pred_box[0], pred_box[1] + 4, f\"Pred: {pred_label}\", color='red')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # Load m√¥ h√¨nh t·ªët nh·∫•t tr∆∞·ªõc khi ƒë√°nh gi√°\n",
    "# model.load_state_dict(torch.load('/kaggle/input/hehe123/resnet100_earlystop_best.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # ƒê√°nh gi√° v√† tr·ª±c quan h√≥a tr√™n t·∫≠p test\n",
    "# print(\"\\nƒê√°nh gi√° tr√™n t·∫≠p test:\")\n",
    "# test_loss= evaluate_model(\n",
    "#     model, test_loader,  compute_bounding_box_loss,generalized_box_iou=generalized_box_iou, iou_threshold=0.5, device=device\n",
    "# )\n",
    "\n",
    "# print(\"Tr·ª±c quan h√≥a m·ªôt s·ªë m·∫´u t·ª´ t·∫≠p test:\")\n",
    "# for idx in range(5):\n",
    "#     if idx < len(test_dataset):\n",
    "#         print(f\"M·∫´u test {idx}:\")\n",
    "#         visualize_predictions(model, test_dataset, idx=idx, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16938e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:21:52.395231Z",
     "iopub.status.busy": "2025-05-16T17:21:52.394924Z",
     "iopub.status.idle": "2025-05-16T17:21:52.534588Z",
     "shell.execute_reply": "2025-05-16T17:21:52.533495Z"
    },
    "papermill": {
     "duration": 0.217346,
     "end_time": "2025-05-16T17:21:52.535784",
     "exception": true,
     "start_time": "2025-05-16T17:21:52.318438",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/1326798179.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# V·∫Ω ƒë·ªì th·ªã loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training vs Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# V·∫Ω ƒë·ªì th·ªã loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d5e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a388ca0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T17:16:26.867273Z",
     "iopub.status.busy": "2025-05-16T17:16:26.866602Z",
     "iopub.status.idle": "2025-05-16T17:16:27.429116Z",
     "shell.execute_reply": "2025-05-16T17:16:27.428370Z",
     "shell.execute_reply.started": "2025-05-16T17:16:26.867249Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ======= C·∫§U H√åNH =======\n",
    "input_dir = \"/Users/minhkha/Downloads/images-2.jpeg\"\n",
    "output_dir = \"/Users/minhkha/Desktop/minh_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ======= TH√îNG S·ªê CHU·∫®N H√ìA (n·∫øu d√πng ImageNet) =======\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ======= LOAD M√î H√åNH =======\n",
    "model = ResNetDetector()\n",
    "model.load_state_dict(torch.load('/Users/minhkha/Downloads/cs231-official.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ======= TI·ªÄN X·ª¨ L√ù ·∫¢NH =======\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# ======= L·∫∂P QUA C√ÅC ·∫¢NH =======\n",
    "# for img_file in os.listdir(input_dir):\n",
    "#     if not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#         continue\n",
    "\n",
    "img_path = input_dir\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "image_resized = image.resize((224, 224))  # ƒë·ªÉ v·∫Ω l·∫°i\n",
    "\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        pred_box, pred_cls = model(input_tensor)\n",
    "        pred_label = pred_cls[0].argmax().item()\n",
    "        pred_box = pred_box[0].cpu().numpy()  # [x1, y1, x2, y2]\n",
    "\n",
    "    # V·∫º K·∫æT QU·∫¢\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(image_resized)\n",
    "\n",
    "rect = patches.Rectangle(\n",
    "        (pred_box[0], pred_box[1]),\n",
    "        pred_box[2] - pred_box[0],\n",
    "        pred_box[3] - pred_box[1],\n",
    "        linewidth=2, edgecolor='red', facecolor='none'\n",
    "    )\n",
    "ax.add_patch(rect)\n",
    "ax.text(pred_box[0], pred_box[1] + 4, f\"Pred: {pred_label}\", color='green')\n",
    "ax.axis(\"off\")\n",
    "img_name = os.path.basename(img_path)\n",
    "output_path = os.path.join(output_dir, img_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.title(f\"Pred: {pred_label}\")\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7122831,
     "sourceId": 11376892,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7438200,
     "sourceId": 11838935,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 51.397918,
   "end_time": "2025-05-16T17:21:55.683153",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-16T17:21:04.285235",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
